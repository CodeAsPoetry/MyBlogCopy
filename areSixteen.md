---
title: Are Sixteen Heads Really Better than One
date: 2019-08-08 15:48:30
categories: 自然语言处理
tags: 论文笔记
mathjax: true
---

## 摘要

注意力机制被强力广泛应用于神经模型通过将信息片段加权平均来获得独自贡献权重从而作出预测的领域。特别是多头注意成为了目前许多最先进的自然语言处理模型的驱动力，包括基于机器翻译的Transformer模型和BERT。这些模型并行地应用多头注意力机制，其中每一个注意头潜在地聚焦输入的不同部分，这使得它比起单个简单的平均加权更可能地表示复杂函数。本论文中，我们惊奇地发现，通过使用多头机制训练好的模型，在测试阶段，即使相当大比例的注意头被移除，性能也没有受到显著影响。<!--more-->事实上，有些层甚至可以被减少到单头。我们进一步研究了用于修剪模型的贪婪算法,以及从中可以获得的潜在速度提升、内存效率和准确性改进。最后,我们分析了模型的哪些部分更依赖于多头的实验结果,并提供了先兆证据,证明了在动态的训练过程中，多头注意力带来的收益在起着一定的作用。

## 1. 介绍

Transformers，作为最先进性能的模型，被广泛应用于各种NLP任务中，包括但不限于机器翻译、问题回答、文本分类和语义角色标注等。其架构改进的核心，Transformer使用多头注意扩展了标准注意机制，即它的注意机制是通过使用${N_h}$个注意头独立计算。

## 2. 背景：注意，多头注意，掩码



## 3. 所有的注意头都是重要的吗？



### 3.1 实验设定



### 3.2 消蚀单头

为了解一个特别的注意头 $h$ ，我们掩盖掉这个头后评估模型的性能(例如用0代替 $Att_h(x)$ )。如果没有 $h$ 的模型性能显著低于全模型，那说明 $h$ 是相当重要的；相反，如果性能是可比较的，则 $h$ 对于给定的剩余模型来说就是冗余的。

图1a和1b，依据掩盖每个头后所导致的模型分数的变化，分别展示了对于WMT和BERT的头的分布。观察到绝大多数的注意头可以被移除而不会带来与原始模型分数的较大偏差。惊奇的是，在某些情况，移除注意头会导致BLEU或者准确率的上升。

为了对这些实验结果有更细粒度的观测，着重以放大视角在表1展示WMT模型的编码器自注意层。值得注意的是，当他们被从模型中移除时，96个头中只有8个会导致在性能上的可观变化。在这8个头中，有一半导致更高的BLEU分数。这使得我们得到第一个观测结论：在测试阶段，对于剩余的模型，绝大多数的头是冗余的。

### 3.3 留下一个消蚀其他所有头

这个观测结论引起这样一个问题：超过一个头是真的被需要吗？因此，在单层内，我们除了留下一个头外，其余所有头都被移除，来比较性能差异。在表2和表3中，我们得到了模型中每一层最好的分数，该分数是由减少整个层到一个最重要的单头所产生的。

经观测发现，对于大多数的层，在测试阶段单个头的确是足够的，即使网络被训练时是12或者16个头。这是非常有意义的，因为这些层可以被减少到单头注意，只有vanilla注意层 $\frac{1}{16}$ ($\frac{1}{12}$) 的参数数量。然而，一些层的确需要多个注意头；例如，将WMT的编解码最后一个注意层用单头替代，将会在性能上导致至少13.5个BLEU点的下降。我们随后在第5节分析不同的模型组件对多头的依赖。

### 3.4 跨数据集重要的头相同吗？

之前两个实验是包含着一个警告的，即实验结果只在具体(或者小的)测试集上加以了验证，这就伴随着在其他数据集上泛化性的质疑。首先要去了解是否有些头是普遍重要的，我们在第二个、域外的测试集进行相同的消蚀研究。其次，我们考虑对于BERT采用MNLI的误匹配验证集和对于WMT模型采用MTNT英语到法语测试集。这两种配合的目的是分别为她们任务产生可对比的、域外的测试集。

我们在这些数据集中的每一个进行如3.2节一样的消蚀实验，结果记录在图2a和图2b中。我们发现在两个数据集中移除一个头所造成的影响之间有正向的大于0.5的相关性(p<001)。并且，在一个领域中会对性能有很高影响力的头往往在另一个数据集中也有同样影响，这表明，来自3.2节的最重要的头的的确确是普遍重要的。

## 4. 迭代修剪注意头

在我们的消蚀实验中(3.2和3.3)，仅观测了在一个单层内移除一个或多个头所带来的影响，并没有考虑同时替换掉两个或多个层会发生什么情况。为了测试跨越整个模型对多个头剪枝所造成的混合影响，我们根据代理重要性分数(下边说明)对模型的所有注意头进行排序，然后逐个移除。我们使用迭代的、贪婪的方法去避免组合搜索。因为在给定的一定数量的头上，组合搜索是不明智的，会耗费大量时间去评估每一个模型。

### 4.1 对于剪枝的头重要性评分

作为对头重要性的代理分数，我们可以用定义在2.3节的掩盖变量 $\xi_h$ 对模型的预期敏感度表示：

$$I_h = \mathbb{E}_{x\thicksim X} |\frac{\partial \mathcal L(x) }{\partial \xi_h}|$$ 

其中，$X$ 是数据分布，$\mathcal L(x)$ 是样本 $x$ 的损失。 直观上，如果 $I_h$ 值较大，那么改变 $\xi_h$ ，很可能对模型有较大的影响。尤其是，我们发现绝对值是至关重要的，要避免那些具有很高的负向或正向影响的数据点在求和过程中使其他点无效。将公式1代入公式2，应用链式求导法则，我们得到 $I_h$ 的最终表达：

$$I_h = \mathbb{E}_{x\thicksim X} |Att_h{(x)^T} \frac{\partial \mathcal L(x) }{\partial Att_h(x)}|$$

这个公式使得回忆起关于神经网络剪枝的文献资源，尤其是，这与Mol等人提出的泰勒展开方法是等效的。

考虑到性能，评估 $I_h$ 只要求进行一次前馈和反馈过程，因此这并不会比训练慢。实际上，我们采用训练集或者其中的一个子集计算期望值。按照Mol等人的推荐，我们使用 $l_2$ 正则层对重要性评分正则化。

### 4.2 剪枝对BLEU/Accuracy影响

图3a(WMT)和图3b(BERT)描述了每步以 $I_h$ 递增的顺序逐步移除10%的注意头从而对模型性能所造成的影响。我们也尝试了与3.2节(虚线中)不同的评分所决定的修剪顺序进行剪枝，发现使用 $I_h$ 的方法是快速的，并且产出更好的实验结果。

我们观测到，此方法分别允许从WMT剪掉20%的头，从BERT剪掉40%的头，而不会导致引人注意的负面影响。但进一步修剪时，模型性能将急剧下降。这意味着在不承受大量性能损失代价的条件下，模型是不可以被完完全全地剪到一个不经再训练的单头。

### 4.3 剪枝对效率的影响

在下游任务的性能上，剪头具有固有的优势。整个模型的每一注意层的每一个头的所包含的参数量占模型整体参数量相当的比例(WMT约为6.25%，BERT约为8.34%)，换个角度，在我们的两个模型中，将近有三分之一的参数量是由跨越所有层的多头注意贡献的。这也是将模型部署到内存受限的设备中面临的主要问题。

然而，我们发现真实地修剪头(而不仅仅是掩盖)可以在推理速度上得到可观的提升。表4提供了BERT原模型和修剪了全部注意头50%比例的模型每秒处理的样本数的对比结果。实验在两台不同机器上进行，都装备GeForce GTX 1080Ti GPUs。 每个实验在每台机器上重复做3次 (那对于每次设定，将有6组数据)。我们发现，剪掉一半的模型注意头，对于更大的批处理大小可以加速约17.5%的推理速度。

## 5. 多头何时重要？机器翻译案例

正如表2所示，不是所有的多头注意层可以被减少到一个单个的注意头而不显著影响性能。为了更好地了解基于transformer翻译模型的每个部分在多大程度上依赖多头机制，我们独立地在每一种注意类型的结构上(编码器到编码器，编码器到解码器，解码器到解码器)重复进行4节中贪婪的修剪实验。

图4展示了当从编解码注意层修剪头时，性能下降得更快。特别是当从编解码注意层剪掉超过60%的注意头时，将导致灾难性的性能下降，而编码器注意层和解码器注意层依然可以产生合理的翻译(BLEU分数大约30)，即使仅有原来模型20%的注意头。换句话说，编解码注意层比自注意层更依赖多头机制。

## 6. 训练期间头动态重要性

之前章节告诉我们在已训练的模型中一些头比起另一些头是更重要的。为了对训练期间头部重要性的动态变化有更多了解，我们在每一代进行相同的4.2节中的增量修剪实验。我们在德语到英语翻译小的IWSLT2014数据集上训练小版本的WMT模型(6层，每层8个头)，进行这个实验。我们把这个模型叫做IWSLT。

图5显示，对于每一程度的修剪(对应原模型增量修剪10%)，每一代模型分数(在newstest2013)的演变历程。为了更好的阅读性，我们对迭代数进行对数放缩，并且在10代以后每5代记录分数。为了使分数在跨代之间有直观的比较性，Y轴表示每代对应于未修剪模型的BLEU分数的相对下降比例。值得注意的是，我们发现其中的两种不同的机制：在相当早期的迭代次数中(特别是1代和2代)，性能随着修剪比例线性下降，如性能的相对下降与 $I_h$ 无关，这表明绝大多数的头都差不多重要；迭代次数10代以后，出现了不重要的头的集中，这些头可以被修剪，且可以达到原始BLEU分数的85%到90%(这些头可以达到总头数40%的比例)。

这表明重要的头在训练早期(但不是立即)被确定。 训练的两个过程符合Shwartz和Tishby的分析。根据此文献，神经网络的训练过程可以被分解为两个过程，一个是“经验风险最小化”阶段，模型最大化中间特征向量表示与标签的互信息；一个是“压缩”阶段，与输入的互信息最小化。这个现象更为原理性的探索研究留待将来工作。

## 7. 相关工作

略......

## 8. 结论

我们观测到多头注意并不总是会在全局程度体现它理论上优于vanilla注意机制的表征能力。具体来说，我们通过丰富的实验设置说明这一点，包括，从训练好的transformer模型中移除多个头，在测试性能上不会显著下降；并且一些层可以被减少到只有一个头。此外，我们在机器翻译模型中展示编解码注意层比自注意层更依赖多头机制，并且提出了每个头的相对重要性在训练早期阶段被确定的证据。我们希望这些观测结果能够使我们更好地理解多头注意，可以促使模型更有效地设计参数和注意机制。

























