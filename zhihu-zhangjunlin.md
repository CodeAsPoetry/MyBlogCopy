---
title: 知乎张俊林文章阅读笔记
date: 2020-01-19 16:51:49
categories: 自然语言处理
tags: 阅读笔记
mathjax: true
---

张俊林大佬在知乎的 深度学习前沿笔记 专栏，让我第一次看技术博客看high了，遂将有关NLP深度学习模型的文章逐字逐句精读一遍，哇，真的太惊艳，万万没想到大佬对哲学和历史也是颇有见地，最关键的是各种梗玩转得炉火纯青。这几篇文章私以为足够建立起NLP深度学习模型的全面框架，包括技术路线、科研论文和应用进展。下面针对这几篇论文记录一下自己的阅读笔记和NLP的心路历程。<!--more-->

[深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)

[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

[放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)

[效果惊人的GPT 2.0模型：它告诉了我们什么](https://zhuanlan.zhihu.com/p/56865533)

[关于百度ERNIE及将知识图谱引入Bert](https://zhuanlan.zhihu.com/p/59503959)

[Bert时代的创新：Bert应用模式比较及其它](https://zhuanlan.zhihu.com/p/65470719)

[Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展](https://zhuanlan.zhihu.com/p/68446772)

[XLNet:运行机制及和Bert的异同比较](https://zhuanlan.zhihu.com/p/70257427)

[天空之城：拉马努金式思维训练法](https://zhuanlan.zhihu.com/p/51934140)

引子：

开头必应用文章一段节选：

“我们人数众多----且日益越来越多-----的算法工程师兄弟，或者姐妹们，虽然整日睡眼惺忪蓬头垢面，但当披上格子战袍那一刻，日渐退后的发际线沐浴在拥堵不堪的西二旗和后厂村路的朝阳中，心中的战斗号角即会吹起，立刻精神百倍鸡血满满地投入到调参掉包等令人激动的炼丹伟大事业第一线：GPU即是我们的三昧真火，Tensorflow就是我们的炼丹炉，LSTM、ResNet就是我们的灵芝草,BN、Attention就是我们的万能调料…… 虽无古炼丹房之仙雾缭绕，却胸怀“十步杀一人，千里不留行”之小小骄傲…….只有当我们意识到，我们以及人数更为众多的工程师们的梦想和躯体的保鲜期只能勉强支撑35年的时候，也许我们才能够真正摆正自己在这个残酷社会中的位置：我们其实与蒸汽机时代的投煤工，嗡嗡作响纺织机旁的纺织工，大航海时代的划桨手并无什么本质的不同。我们其实才是需要逃离埃及苦地的新时代犹太人，但是也许，到最终会发现，世上并不存在能借力神灵之手的摩西引路。”

2019年6月份意气风发地走出校园，以作为NLP算法工程师的无限荣光献身AI事业，经过半年的炼丹，当看到如此一段，猛然发现是到了该给自己找一个明确定位的时候了，深刻体会到自己作为一个底层炼丹师的肤浅和蛮干。闲暇之余，将爷爷留给爸爸的财富和爸爸目前给我留下的财富，分别按照1993年和2014年的物价标准折合计算了一下，再考虑一下目前的房价和赡养上一代以及培养下一代的经济负担，发现如果我还是目前这个走势，估计不如上辈人，不禁汗颜。

正文：

NLP领域的深度学习模型的大致路线：

序列模型(RNN，LSTM，GRU)——序列模型加注意力模型——自注意机制——Transformer——GPT1.0/GPT2.0/BERT——Transformer-XL——XLNet，最近模型也有往知识图谱方向融合的趋势。

目前个人技术只达到BERT，需要研读复现一波Transformer-XL和XLNet论文，知识图谱倒已经有了一定拓展。关于Transformer-XL，解读博客 [Transformer-XL解读（论文 + PyTorch源码）](https://blog.csdn.net/magical_bubble/article/details/89060213)

深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。

可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。

当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。

将Attention机制看作一种软寻址: Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。

单从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。这两者再综合起来，排序结果应该是Transformer>CNN>RNN。从速度和效果折衷的角度看，对于工业界实用化应用，我的感觉在特征抽取器选择方面配置Transformer base是个较好的选择。

从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做。

对于句子匹配类任务，或者说是输入由多个不同组成部分构成的NLP任务，那么在应用Bert的时候，采用Fine-tuning效果是要明显好于特征集成模式的。所以遇到这种类型的任务，不用犹豫，直接上Fine-tuning没有大错。而对于其它类型的任务，在应用Bert的时候，Fine-tuning模式要稍好于特征集成模式，或者两者效果差不多。再简练点的话，结论是：对于Bert应用，安全稳妥的做法是，建议采取Fine-tuning的模式，而不是特征集成的模式。

对于句子匹配等多输入的NLP任务，直接使用Bert高层的[CLS]标记作为输出的信息基础，这是效果最好的，也是最简单的模式。对于序列标注类任务，可能多层特征融合更适合应用场景，但是在融合各层特征时，要做细致些。对于单句分类等其它任务，因为没有更多的工作或者实验来说明这个问题，所以尚未能下明确的结论，这块还需要后续更多的验证工作。

Bert时代的可能NLP创新路径

* 在完全不依赖Bert的基础上，提出一个与Bert效果相当或者更好的新模型或新方法
* 不考虑模型创新，可以利用Bert预训练模型，直接去做各种应用，以实证Bert在各种领域是有效果的
* 通过各种偏实验性的研究，以更深入地了解Bert的特性，对Bert及Transformer有很深刻的了解
* 直接改进Bert模型。针对Bert目前还做得不太好的地方，改进优化它，或者改造使得它能够适用更广的应用范围
* 想出那些在Bert基础之上，又看上去与Bert无关的改进，期待新技术叠加到Bert上去之后，新方法仍然有效
* 找Bert做不好的任务或应用领域，参考Bert的基本思想，是很有可能引入大的改进模型的

XLNet起作用的，如果宏观归纳一下，共有三个因素；

1. 与Bert采取De-noising Autoencoder方式不同的新的预训练目标：Permutation Language Model(简称PLM)；这个可以理解为在自回归LM模式下，如何采取具体手段，来融入双向语言模型。这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。
2. 引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的；
3. 加大增加了预训练阶段使用的数据规模；Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。这个明显走的是GPT2.0的路线。

XLNet综合而言，效果是优于Bert的，尤其是在长文档类型任务，效果提升明显。如果进一步拆解的话，因为对比实验不足，只能做个粗略的结论：预训练数据量的提升，大概带来30%左右的性能提升，其它两个模型因素带来剩余的大约70%的性能提升。当然，这个主要指的是XLNet性能提升比较明显的阅读理解类任务而言。对于其它类型任务，感觉Transformer XL的因素贡献估计不会太大，主要应该是其它两个因素在起作用。

