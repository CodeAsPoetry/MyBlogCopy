---
title: BERT Pre-training of Deep Bidirectional Transformers for Language Understanding
date: 2019-08-14 21:03:21
categories: 自然语言处理
tags: 论文笔记
mathjax: true
---

## 摘要

我们介绍了一个全新的语言表示模型BERT，这是利用Transformer双向编码表示的缩写简称。与最近的语言表示模型不同，BERT被设计为来自未标记文本的由所有层从左右两边的上下文联合决定的预训练的深度双向表示模型。因此，仅带有一个额外的输出层的预训练BERT模型可以被微调，从而在一系列广泛的任务中产生最先进的模型，比如问答和语言推理，无需大量的针对具体任务对模型架构作出修改。<!--more-->

BERT，概念上简单，经验上有效。它在11项自然语言处理任务上保持最新的先进成果，包括将GLUE分数推高到80.5%(7.7%的绝对提高)，MultiNLI准确率达到86.7%(4.6%的绝对提升)，在SQuAD1.1版问答任务测试集上，将F1提高至93.2(1.5的绝对提高)，在SQuAD2.0版测试集上F1达到83.1(5.1的绝对提升)。

## 1. 介绍



## 2. 相关工作



## 3. BERT

我们在这一节介绍BERT和它具体实现。我们架构中有两步：预训练和微调。在预训练阶段，模型在不同的预训练任务的未标记数据中被训练。微调阶段，BERT模型首先用预训练的参数初始化，所有的参数都会通过来自下游任务的有标记数据来微调。每一个下游任务由独立的微调模型，即使它们用同一个预训练模型初始化。图1问答示例在这节将用来作为示范用例。

BERT一个显著的特点是跨越不同任务的结构统一性。在预训练模型结构和最终下游任务模型结构间有最小的差异。

* 模型架构 

  BERT的模型架构是一个基于Vaswani等人描述的在tensor2tensor库中发布的原始实现的多层双向Transfomer编码器。因为Transformers被普遍使用且我们的实现与原始的一样，所以我们省略关于模型架构众所周知的背景，查看Vaswani等人的说明或更出色的指导，如“注释Transformer”。
  
  在这项工作中，我们将层数(即Transformer块)记为 $L$ ，隐状态大小记为 $H$ ，自注意头的数目记为 $A$ 。我们主要在两个不同大小的模型上展示实验结果：$BERT_{BASE}$ ($L=12$，$H=768$，$A=12$ ，参数总量110M)，和$BERT_{LARGE}$ ($L=24$ ， $H=1024$ ，$A=16$ 参数总量340M)。
  
  出于比较的目的，$BERT_{BASE}$ 被选择与OpenAI GPT有相同的模型大小。关键是，BERT的Transformer单元使用双向自注意，而GPT的Transformer单元使用限制的自注意，即每个token仅能使用它左边的文本。

* 输入/输出表示

  为了使BERT可以处理丰富的下游任务，我们的输入表征要能够在一个token序列中明确地表示一个单句和一个句对(例如，<问答>)。整个这个工作中，所谓一个句子可以是任意相连着的文本span，而不一定非得是真实语用的句子。所谓对的一个序列，指的是输入给BERT的一个token序列，它可能是一个单句，也可能是两个句子打包在一块。

  我们使用一个有30000个token大小的词表。每个序列的第一个token总是一个特殊的分类token ([CLS])。对应这个token的最终隐状态被用作分类任务中总计序列表征。句对被一起打包到一个单个的序列。我们用两种方法区分这些句子。第一，我们用一个特殊的token ([SEP]) 分隔它们。第二，我们添加一个可以被学习的嵌入层，对于每一个token表示它属于句子A还是句子B。  如图1所示，我们用 $E$ 表示输入嵌入，特殊 [CLS] token 的最终隐向量用 $C \in \mathcal R^H$ 表示，第 $i$ 个输入 token 的最终隐向量用 $T_i \in \mathcal R^H$ 表示。

  对于一个给定的token，它的输入表示是由相应的token、其所在段落和位置嵌入通过求和构建。这个构建的可视化过程如图2所示。

### 3.1 预训练BERT

不像Peters和Radford等人那样，我们并没有使用传统的由左到右或者由右到左的语言模型去预训练BERT。相反，我们使用两个无监督的任务去预训练，这将在下文章节介绍，这个步骤如图1左半部分所示。

* Task #1: Masked LM 遮盖语言模型

  直觉上，是可以合理认为一个深度双向模型比起一个从左到右的模型或者将从左到右和从右到左浅层合并的模型更加确切有效。不幸的是，标准受限的语言模型仅可以被从左到右或者从右到左训练，因为双向调节允许每个单词可以间接地看到“它自己”，并且模型可以在一个多层的上下文中琐细地预测目标词。

  为了训练一个深度双向特征表示，我们简单随机地遮盖掉一定比例的token输入，然后预测那些被遮盖掉的token。我们把这个过程叫做MLM，尽管在文献中被叫做完形填空任务。在该场景下，对应被掩盖掉的tokens的最终隐状态向量被送入到对应词表的softmax输出层，作为一个标准的语言模型。在我们整个实验中，随机掩盖掉每个序列中所有wordPiece tokens 15%的比例。与降燥自动编码器不同，我们只预测被掩盖掉的词，而不是重构整个输入。

  虽然这允许我们得到一个双向预训练模型，但是一个副作用是我们同时也创造着在预训练和微调之间的误匹配，因为  [MASK] 并不会在微调阶段出现。为了缓和这个问题，我们并不总是用真实的 [MASK] token 代替想要掩盖的词。训练数据生成器随机选择 15% 的 token 位置用来预测。如果第 $i$ 的 token 被选择，我们将以 80% 的概率用 [MASK] 代替；将以 10% 的概率用一个随机 token 代替；以 10% 的概率不作改变。然后，$T_i$  通过交叉熵损失被用于预测原始的 token 。我们会在附录C.2比较这个过程的各种变体。

* Task #2: Next Sentence Prediction (NSP) 下一个句子预测

  许多重要的下游任务，如问答和自然语言推理，都是基于对两个句子间关系的理解，而这并不会直接被语言模型捕捉到。为了训练一个能够理解句子关系的模型，我们在一个可以由任何单语语料中生成的下一个句子预测的二分类任务。具体来说，对于每个预训练样本，选择句子A和句子B，有50%的概率，B的确是真实的A的下一条句子(标记为 IsNext)，有50%的概率，B只是从语料中随机选择的一个句子(标记为 NotNext)。如图1所示，$C$ 被用于下个句子预测(NSP)。虽然比较简单，我们会在5.1节说明针对下个句子预测的任务的预训练对QA和NLI都非常有益。

  NSP任务与Jernite、Logeswaran、Lee等人的表征学习目标比较相关。但是，在之前的工作中，只有句子的嵌入向量表示被转移到下游任务中，而BERT用预训练后的所有参数来初始化具体任务的模型。

* Pre-training data 预训练数据

  预训练过程极大地使用在语言模型预训练的存在的文献语料。对于预训练语料，我们使用BooksCorpus (800M单词)和英文维基百科 (2500M 单词)。对于维基百科，我们只抽取文本文章，忽略列表、表格和头信息。关键是要使用文档级别的语料，而不是一些如 Billion Word Benchmark 等随机句子级别的语料，以便能够抽取出长的邻接的序列。

### 3.2 微调BERT

微调是直白简单的，因为Transformer中的自注意机制就允许BERT对许多下游任务建模，无论是单文本还是文本对，只需转换合适的输入输出。对于涉及到文本对，一个普通的模式是在应用双向交叉注意前，先分别单独对文本对编码，如Parikh、Seo等人那样。相反，BERT使用自注意机制将这两步统一起来，使用包括两句间的双向交叉注意的自注意机制有效地对一个合并的文本对编码。

对于每一个任务，我们简单地在BERT中插入具体任务相关的输入和输出，并且端到端微调所有参数。在输入中，句A和句B与下面几种情况是类似的：(1). 句对释义，(2). 限定假设前提对，(3).  QA任务中的问答-文章对，和(4). 在文本分类或者序列标注可以看作退化版本的文本和空集对。在输出中，对于 token 级别的任务，token的特征表示向量被送进一个输出层，例如序列标注或者问答，并且 [CLS] 表征被送入到分类的输出层，例如语义蕴含或者情感分析。

与预训练相比，微调成本是相对较低的。从同样的预训练模型开始，在一块云TPU上，最多花费1个小时，或者在1块GPU上花费数几小时，论文中所有实验结果都可以被重现。我们将在第4节相应部分介绍具体任务细节。更多的细节可以在附录 A.5 中加以说明。

## 4. 实验



## 5. 消蚀研究



## 6. 结论

最近由针对语言模型的迁移学习的经验表明丰富的、无监督的预训练是许多语言理解系统的重要组成部分。特别是，这些结果可以通过深度单向结构的模型使具有低资源的任务获益。我们的主要贡献是进一步扩展这些成果到深度双向结构中，允许整个同样的预训练模型能够成功地处理一整套NLP任务。

