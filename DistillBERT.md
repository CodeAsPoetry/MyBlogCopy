---
title: DistillBERT, a distilled version of BERT smaller, faster, cheaper and lighter
date: 2019-10-10 17:03:33
categories: 自然语言处理
tags: 论文笔记
mathjax: true
---

## 摘要

尽管在自然语言处理领域中从大规模的预训练模型中迁移学习越来越普遍，但在实时并且/或者计算受限的约束下，训练和推断的负担依然很具有挑战。在这里，我们提出一个预训练更小的通用语言表征模型的方法，称为“蒸馏BERT”，它可以像它的巨大的同伴模型一样，被微调到广泛的任务中，且得到很好的性能。<!--more-->相对于绝大数之前的工作都在研究利用蒸馏学习构建具体任务的模型不同，我们在预训练阶段利用知识蒸馏，并且展示了其可行性，即减少了BERT模型的40%的尺寸，依然得到了其在语言理解能力的97%性能，并且加快了60%。为了在预训练阶段，利用由大模型学习得到的归纳偏差，我们引入一个三项损失的组合，包括语言模型、蒸馏学习和余弦距离损失。我们更小、更快、更轻的模型预训练成本更便宜，并且我们在概念验证实验和比较型设备研究中展示了其设备计算性能。

## 1. 介绍



## 2. 知识蒸馏

**知识蒸馏**是一种模型压缩技术，其中，一个紧凑的学生模型被训练重现作为教师的一个更大的模型或者一组模型的行为。

在监督训练中，

## 3. 蒸馏BERT：一种BERT的蒸馏版本



## 4. 实验



## 5. 相关工作



## 6. 结论和将来工作



